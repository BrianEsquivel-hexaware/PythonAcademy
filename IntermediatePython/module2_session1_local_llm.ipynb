{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fd3924",
   "metadata": {},
   "source": [
    "## What an LLM is (in practice)\n",
    "\n",
    "A **Large Language Model (LLM)** is a neural network trained to predict the next token (piece of text) given prior tokens. That simple objective turns into useful capabilities—summarization, extraction, Q&A, code generation—because the model has learned statistical structure from massive text/code corpora.\n",
    "\n",
    "In an app development, you usually interact with an LLM in one of two ways:\n",
    "\n",
    "* **Chat mode**: a sequence of `{role, content}` messages (system/developer/user/assistant).\n",
    "* **Completion mode**: one big prompt string (less common now, but still used).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9995658",
   "metadata": {},
   "source": [
    "## Local LLMs: what changes vs “cloud LLMs”\n",
    "\n",
    "A **local LLM** means the model weights run on *your* hardware (laptop/desktop/server), so:\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* **Cost control**: no per-token API bill; you mainly “pay” in hardware + electricity.\n",
    "* **Privacy & data locality**: sensitive docs never have to leave your machine.\n",
    "* **Offline / air-gapped**: possible for secure environments. ([Ollama][1])\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* **Throughput + latency** depends on your CPU/GPU and quantization choices.\n",
    "* **Model size constraints**: big models can be impractical without a strong GPU / lots of RAM/VRAM.\n",
    "* **Ops overhead**: downloads, storage, model management, and performance tuning.\n",
    "\n",
    "Here is an article I recommend on this: [You're using your local LLM wrong if you're prompting it like a cloud LLM](https://www.xda-developers.com/youre-using-local-llm-wrong-if-youre-prompting-it-like-cloud-llm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbf3e0",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "**Ollama** is a tool that lets you run Large Language Models directly on your own computer, no cloud required. No subscriptions. No sending your data to Big Tech's servers. It makes local models easy to:\n",
    "\n",
    "* download/pull and run from CLI\n",
    "* serve over a **local HTTP API** (default `http://localhost:11434/api`) ([Ollama Documentation][2])\n",
    "* integrate into apps with official client libraries (Python/JS) ([Ollama Documentation][2])\n",
    "\n",
    "It also supports GPU acceleration across common stacks (NVIDIA CUDA, AMD ROCm, Apple Metal, and experimental Vulkan paths). ([Ollama Documentation][3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffce475",
   "metadata": {},
   "source": [
    "## Costs: what you actually pay for\n",
    "\n",
    "### 1) Ollama software cost\n",
    "\n",
    "Ollama has a **Free** plan for running models on your own hardware; paid tiers mainly relate to *cloud* usage and extras. ([Ollama][1])\n",
    "\n",
    "### 2) Local inference cost drivers (the real ones)\n",
    "\n",
    "* **Hardware**: GPU/VRAM (or CPU/RAM) is the main limiter.\n",
    "* **Electricity/thermals**: sustained inference draws power; laptops may throttle.\n",
    "* **Time cost**: slower tokens/sec can be more expensive than cloud for “human time.”\n",
    "\n",
    "### 3) Why quantization matters (and why everyone talks about it)\n",
    "\n",
    "Local setups often rely on **quantized** weights (e.g., 4–8 bit) to reduce memory and run bigger models. A recent empirical study focusing on llama.cpp quantization highlights that different quantization schemes trade off **quality vs speed vs memory** and should be chosen based on your hardware and task needs. ([arXiv][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8483c1",
   "metadata": {},
   "source": [
    "## Resources you need\n",
    "\n",
    "Think in three knobs:\n",
    "\n",
    "1. **Model size** (e.g., 3B, 8B, 14B parameters)\n",
    "2. **Quantization level** (e.g., 4-bit vs 8-bit)\n",
    "3. **Context length** (how many tokens you keep “in memory”)\n",
    "\n",
    "**Context length is a hidden VRAM tax.** Ollama even defaults context length based on VRAM tiers (e.g., <24 GiB → 4k context; 24–48 GiB → 32k; ≥48 GiB → 256k). Larger context increases memory requirements. ([Ollama Documentation][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0571c",
   "metadata": {},
   "source": [
    "## Prompt techniques that matter *more* for local/smaller LLMs\n",
    "\n",
    "Smaller local models are less “instruction-sticky” than frontier cloud models, so you get better results by being **more structured** and **more constrained**.\n",
    "\n",
    "### 1) Use *tight* instructions + explicit output schemas\n",
    "\n",
    "Local models drift more. Give:\n",
    "\n",
    "* a short role (“You are an information extraction engine.”)\n",
    "* constraints (“Return valid JSON only.”)\n",
    "* a schema example (keys, types)\n",
    "* a **single** success criterion (what “good” looks like)\n",
    "\n",
    "### 2) Few-shot, but *small* and highly relevant\n",
    "\n",
    "Few-shot prompting (2–5 examples) often boosts reliability because you’re showing the exact pattern you want. Keep examples short and format-identical to the target output. ([promptingguide.ai][6])\n",
    "\n",
    "### 3) Delimiters and sectioning (reduce confusion)\n",
    "\n",
    "Use clear separators like:\n",
    "\n",
    "* `### Instruction`\n",
    "* `### Input`\n",
    "* `### Output (JSON)`\n",
    "\n",
    "This helps weaker models not mix instructions with data.\n",
    "\n",
    "### 4) Prefer lower temperature + controlled decoding for consistency\n",
    "\n",
    "For extraction/summarization/metadata tasks:\n",
    "\n",
    "* temperature: **0.0–0.3** (start low)\n",
    "* if outputs repeat or ramble, reduce randomness further and add stricter formatting constraints\n",
    "\n",
    "Also: if you’re using chat models via Hugging Face tooling, **chat template / prompt wrapping** mistakes can badly degrade controllability—this bites local setups a lot. ([Hugging Face Forums][7])\n",
    "\n",
    "### 5) Keep context short; “stuffing” harms smaller models faster\n",
    "\n",
    "Instead of pasting huge documents:\n",
    "\n",
    "* chunk text\n",
    "* summarize per chunk\n",
    "* then merge summaries\n",
    "  This is usually better than trying to brute-force long context on a small model (and cheaper in VRAM). ([Ollama Documentation][5])\n",
    "\n",
    "### 6) Use “format-first” prompting for structured tasks\n",
    "\n",
    "A reliable pattern for local LLMs:\n",
    "\n",
    "1. Provide JSON schema\n",
    "2. Provide 1 example input/output\n",
    "3. Provide the real input\n",
    "4. Demand “JSON only” (no prose)\n",
    "\n",
    "This reduces “helpful chatter” and failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ff281",
   "metadata": {},
   "source": [
    "# **Useful Ollama CLI commands**\n",
    "\n",
    "### Help / version\n",
    "\n",
    "* `ollama -h` / `ollama --help` — show commands and flags\n",
    "* `ollama --version` — show installed version\n",
    "\n",
    "### Run & serve (inference)\n",
    "\n",
    "* `ollama run <model>` — run a model interactively (chat in terminal)\n",
    "\n",
    "  * Example: `ollama run qwen2.5:3b`\n",
    "* `ollama serve` (alias: `start`) — start the local Ollama server (HTTP API)\n",
    "* `ollama stop <model>` — stop a running model\n",
    "* `ollama ps` — list running models (what’s loaded / active)\n",
    "\n",
    "### Model management\n",
    "\n",
    "* `ollama pull <model>` — download a model from a registry \n",
    "* `ollama list` (often `ollama ls`) — list models you have downloaded \n",
    "* `ollama show <model>` — show model info/config (params, context length, etc.) \n",
    "* `ollama rm <model>` — remove a local model \n",
    "* `ollama cp <src> <dst>` — copy/duplicate a model locally (handy for variants) \n",
    "* `ollama push <model>` — push a model to a registry (requires account/auth) \n",
    "\n",
    "### Custom models / modelfiles\n",
    "\n",
    "* `ollama create <new_model> -f <Modelfile>` — create a custom model definition (system prompt, params, base model, etc.) \n",
    "\n",
    "### Common “day-1” workflow\n",
    "\n",
    "```bash\n",
    "ollama pull qwen2.5:3b\n",
    "ollama run qwen2.5:3b\n",
    "ollama serve\n",
    "ollama ps\n",
    "ollama stop qwen2.5:3b\n",
    "```\n",
    "### References and useful links\n",
    "- [CLI Reference](https://docs.ollama.com/cli)  \n",
    "- [Ollama example](https://docs.radxa.com/en/cubie/a7s/app-dev/ollama-dev/ollama-example) \n",
    "- [Ollama CLI tutorial: Learn to use Ollama in the terminal](https://www.hostinger.com/tutorials/ollama-cli-tutorial) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c8ac7",
   "metadata": {},
   "source": [
    "> Content created by [**Carlos Cruz-Maldonado**](https://www.linkedin.com/in/carloscruzmaldonado/).  \n",
    "> I am available to answer any questions or provide further assistance.   \n",
    "> Feel free to reach out to me at any time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
